{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ba246d",
   "metadata": {},
   "source": [
    "# Grammar dependency notebook\n",
    "Authors\n",
    "https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.2.zip -o stanford-corenlp-4.2.2.zip \n",
    "# !curl https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.2-models-english.jar -o stanford-corenlp-4.2.2-models-english.jar\n",
    "# !7z x stanford-corenlp-4.2.2.zip # windows\n",
    "# !unzip stanford-corenlp-4.2.2.zip# mac / linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://web.stanford.edu/~jurafsky/slp3/14.pdf\n",
    "# https://stanfordnlp.github.io/CoreNLP/index.html    \n",
    "from nltk.parse import DependencyGraph,ProjectiveDependencyParser,NonprojectiveDependencyParser\n",
    "from nltk import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to CoreNLP jar unzipped and model jar\n",
    "jar_path = 'stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
    "models_jar_path = 'stanford-corenlp-4.2.2-models-english.jar'\n",
    "\n",
    "# Initialize StanfordDependency Parser from the path\n",
    "parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n",
    "\n",
    "# Parse the sentence\n",
    "text = 'The monkey is in the tree'\n",
    "\n",
    "result = parser.raw_parse(text)\n",
    "dependency = result.__next__() #bad API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format('Head', 'Head POS','Relation','Dependent', 'Dependent POS'))\n",
    "print (\"-\" * 75)\n",
    "  \n",
    "# Use dependency.triples() to extract the dependency triples in the form\n",
    "# ((head word, head POS), relation, (dependent word, dependent POS))  \n",
    "for dep in list(dependency.triples()):\n",
    "    print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\"\n",
    "         .format(str(dep[0][0]),str(dep[0][1]), str(dep[1]), str(dep[2][0]),str(dep[2][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bec677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = dependency.nx_graph()\n",
    "\n",
    "words = text.split(\" \")\n",
    "labels = {index + 1: words[index] for index in range(len(words))}\n",
    "nx.draw(G, with_labels=True, labels=labels, node_size=2500, node_color='#B5EAD7', font_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b25915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf83941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/index.html\n",
    "    \n",
    "G = nx.Graph(G)\n",
    "centrality = nx.betweenness_centrality(G)\n",
    "print(centrality)\n",
    "\n",
    "print(\"-\"*60)\n",
    "for key in centrality:\n",
    "    print (\"{:<10}|{:<10}\".format(labels[key],str(centrality[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbda527",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "The grammatical complexity of a text can be determined by the degree of dependency between words in the same sentence. Any suggestions on how to calculate a footprint/spectrum of this?\n",
    "\n",
    "La complejidad gramatical de un texto se puede determinar por el grado de dependencia entre palabras de una misma sentencia. ¿alguna sugerencia de como poder calcular una huella/un espectro de esto?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa23860",
   "metadata": {},
   "source": [
    "shall we implement it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import functools\n",
    "\n",
    "romeojulieta = open('romeojulieta.txt','r')\n",
    "text = romeojulieta.read()\n",
    "romeojulietaSentences = re.split(r'(\\.\\W|\\n{2,})', text) # may create empty elements\n",
    "romeojulietaSentences = list(filter(lambda sentence: not re.match(r'^\\W*$', sentence), romeojulietaSentences)) # remove empty elements\n",
    "romeojulietaSentences = list(filter(lambda sentence: not re.match(r'^\\w+$', sentence), romeojulietaSentences)) # remove one word elements\n",
    "\n",
    "romeojulietaCleanSentences = list(map(lambda sentence: re.sub(r'((\\W(?!(\\w)))+.|\\n+)', \" \", sentence).lower(), romeojulietaSentences))\n",
    "for sentence in romeojulietaCleanSentences[0:5]:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f77b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(dict):\n",
    "  values = []\n",
    "  for k,v in dict.items():\n",
    "    values.append(v)\n",
    "  return values\n",
    "\n",
    "def calculateListAverage(list):\n",
    "    return sum(list) / len(list)\n",
    "\n",
    "def calculateDependencyDegree(text):\n",
    "  result = parser.raw_parse(text)\n",
    "  graph = nx.Graph(result.__next__().nx_graph())\n",
    "  centrality = nx.betweenness_centrality(graph)\n",
    "  values = getValues(centrality)\n",
    "  return calculateListAverage(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c006da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "averages = []\n",
    "for sentence in tqdm.tqdm(romeojulietaCleanSentences[0:50]):\n",
    "  degree = calculateDependencyDegree(sentence)\n",
    "  averages.append(degree)\n",
    "\n",
    "print(calculateListAverage(averages))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "MyPy397",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
